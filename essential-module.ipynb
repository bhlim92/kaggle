{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7421538,"sourceType":"datasetVersion","datasetId":4317956}],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install pmdarima\n!pip list\n\nc_data_path =\"/kaggle/input/adp-realdataset/\"\ndata_path=\"./data/\"","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from unittest import skip\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport sklearn \nimport scipy.stats as st\n    \n# help(sklearn)\na = [3,3.5,2, 1.8, 1]\nb = st.norm(3,0.8).rvs(100)\n# print(help(st.norm))\nprint(np.mean(a), np.std(a)) \nr = st.shapiro(a)\nr2 = st.shapiro(b)\nprint(r,r2)\nret =  st.ttest_1samp(a,2.0, alternative='less')\nr_eqv = st.levene(b, a)\nr_eqv2 = st.bartlett(b, a)\nprint(r_eqv,r_eqv2)\nret =  st.ttest_ind(a,b, equal_var=True )\nprint(ret)","metadata":{"execution":{"iopub.execute_input":"2024-03-09T05:43:29.175230Z","iopub.status.busy":"2024-03-09T05:43:29.174868Z","iopub.status.idle":"2024-03-09T05:43:29.188233Z","shell.execute_reply":"2024-03-09T05:43:29.187131Z","shell.execute_reply.started":"2024-03-09T05:43:29.175204Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy, statsmodels\nfrom statsmodels.tsa import arima,api \n\nll = [i for i in dir(scipy.stats) if \"ba\" in i]\nll","metadata":{"execution":{"iopub.execute_input":"2024-03-02T12:03:06.958534Z","iopub.status.busy":"2024-03-02T12:03:06.958039Z","iopub.status.idle":"2024-03-02T12:03:06.967327Z","shell.execute_reply":"2024-03-02T12:03:06.966305Z","shell.execute_reply.started":"2024-03-02T12:03:06.958497Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **4 전처리** ","metadata":{}},{"cell_type":"code","source":"# 기본적인 모듈, EDA시 \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn \nimport statsmodels.tsa\n\n# 전처리 \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\n# pca = PCA( n_components = 4) \n# pca_fit = pca.fit(x)\n# print(\"고유 값 : \", pca.singular_values_)\n# print(\"분산 설명력: \", pca.explained_variance_ratio_)\n\nll = [i for i in dir(sklearn.metrics) if \"error\" in i ]\nstats = [i for i in dir(statsmodels.tsa)]\n# \ndf = pd.read_csv(\"./data/iris.csv\")","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:08:50.085231Z","iopub.status.busy":"2024-04-15T14:08:50.084702Z","iopub.status.idle":"2024-04-15T14:08:50.093782Z","shell.execute_reply":"2024-04-15T14:08:50.092732Z","shell.execute_reply.started":"2024-04-15T14:08:50.085192Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4 머신러닝 문제 푸는 순서 ","metadata":{}},{"cell_type":"markdown","source":"### 4-1  데이터 확인 \n- 독립변수, 종속변수 존재확인, 각 변수의 타입확인(이산형, 범주형), 적용가능한 분석 모델 결정 회귀/분류/비지도학습\n- 종속변수, target , 법주형 ","metadata":{}},{"cell_type":"code","source":"# 변수확인\ndf.info()\n## 결측치 있는지 확인 \ndf.isna().sum()\ndf.head()\n### 목적 변수, 결과 변수 확인 \n\n## 데이터 상관관계 \ndf_corr = df.drop(\"target\", axis = 1).corr()\ndf_corr = df_corr.unstack()\n\n## 상관도가 제일 높은 순서대로.. \ndf_corr = pd.DataFrame(df_corr).reset_index()\ndf_corr.columns = [\"clm1\",\"clm2\",\"corr\"]\ndf_corr = df_corr[df_corr[\"corr\"] != 1]\ndf_corr.sort_values([\"clm1\",\"corr\"], ascending=False).drop_duplicates(\"clm1\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-2 데이터 분할 :\n- 학습, 검정, 평가 세트 80:10:10\n- 교차 검증방법 적용 \n","metadata":{}},{"cell_type":"code","source":"#4-2\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n\nx = df.drop(\"target\", axis=1)\ntargets = df[\"target\"].unique()\ny = df[\"target\"].map({'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2})\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape\ntype(x), type(x_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-3 전처리 , 표준화 또는 정규화 적용 \n- 독립변수의 법주형 자료를 원핫인코딩 \n- 결측치 및 이상치 확인후 처리 ","metadata":{}},{"cell_type":"code","source":"#4-3\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nscaler = StandardScaler()\nscaled_x_train = scaler.fit_transform(x_train)\nscaled_x_test = scaler.transform(x_test)\n\ntype(x), type(x_train), type(scaled_x_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-4 모델학습, 회귀/분류/비지도 학습 중 다양한 알고리즘 \n### 4-4-1. 분류 모델 ","metadata":{}},{"cell_type":"code","source":"# 4-4 분류 \nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.metrics import accuracy_score, precision_score, classification_report\n\nmodel  = LogisticRegression(penalty='none', multi_class = 'multinomial', random_state=92)\nmodel.fit(scaled_x_train, y_train)\npred_train = model.predict(scaled_x_train)\nacc = model.score(scaled_x_train, y_train)\nacc, pred_train\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 분류 모델 종류 \nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB \n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, VotingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n# 비지도 \nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.mixture import GaussianMixture","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-4-2 회귀 모델 ","metadata":{}},{"cell_type":"code","source":"## 4-4 회귀 \n## 회귀 분석 예시 \nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n# x.drop(\"sepal length\", axis = 1, inplace=True)\nmtcars = pd.read_csv(\"./data/mtcars.csv\")\n\nprint(mtcars.info())\nprint(mtcars.isna().sum())\ncorr = mtcars.corr()\nprint(corr)\nimport seaborn as sns \n# fig, ax = plt.subplots(2,1, figsize=(10,10))\n# plt.subplot(2,1,1)\n\nsns.heatmap(corr,  annot=True)\nplt.show()\n# plt.subplot(2,2,2)\nsns.pairplot(mtcars.drop([\"Unnamed: 0\"],axis = 1), corner=True, diag_kind='kde')\nplt.show()              \n\nx = mtcars.drop([\"Unnamed: 0\",\"mpg\"], axis = 1)\ny = mtcars[\"mpg\"]\n\ntrain_x, test_x, train_y, test_y = train_test_split(x,y, random_state=92, test_size=0.2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr= LinearRegression()\ntrain_x.shape, train_y.shape\nlr.fit(train_x, train_y)\n\nr2 = lr.score(train_x, train_y)\nprint(r2) \nprint(lr.coef_, lr.intercept_)\n\npred_test = lr.predict(test_x)\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\nmae = mean_absolute_error(test_y, pred_test)\nmse = mean_squared_error(test_y, pred_test)\nrmse = np.sqrt(mse)\n\nprint(mae, mse, rmse)\n\n# plt.figure(figsize=(10,10))\n# sns.heatmap(corr, annot= True)\n# plt.show()\n# corr.columns.name =\"h_corr\"\n# corr\n\n# corr_v = corr.unstack()\n# # print(type(corr_v))\n# corr_v = pd.DataFrame(corr_v).reset_index()\n# corr_v.columns = [\"clm1\",\"clm2\",\"corr\"]\n# corr_v = corr_v[ corr_v[\"corr\"] != 1.0]\n# corr_v.sort_values([\"clm1\",\"corr\"], ascending=False).drop_duplicates(\"clm1\",keep='first').reset_index(drop=True)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### 회귀 모델 종류 \nfrom sklearn.linear_model import ARDRegression,BayesianRidge,GammaRegressor,Hinge,HuberRegressor, \\\nQuantileRegressor,SGDRegressor, LinearRegression, ElasticNet,Ridge, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, VotingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-4-3 Voting Classifier\n여러 모델을 사용하여 투표하여 최종 결과물을 만들어 내다. ","metadata":{}},{"cell_type":"code","source":"# 4-4-3 voting classifier는 다양한 모델을 사용하여 soft voting, hard voting해서 결과를 만들어낸다. \n\nfrom sklearn.ensemble import VotingClassifier\n# from sklearn.ensemble import VotingRegressor\n# help(VotingRegressor)\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nclf1 = LogisticRegression(multi_class='multinomial', random_state=1)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\ny = np.array([1, 1, 1, 2, 2, 2])\neclf1 = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')\neclf1 = eclf1.fit(X, y)\nprint(eclf1.predict(X))\n# eclf1.named_estimators\nprint(f\"{clf1.fit(X,y).predict(X)}, {clf2.fit(X,y).predict(X)},{eclf1.estimators_[2].fit(X,y).predict(X)}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4-5. 성능 평가\n- 평가세트에 대한 최종 모델을 적용 \n- 평가세트에 대한 정확도를 머신러닝 최종 모델 적용","metadata":{}},{"cell_type":"code","source":"# 4-5 , 성능 평가 \nfrom sklearn.metrics import accuracy_score, precision_score, classification_report, mean_absolute_error, mean_squared_error\n\npred_test= model.predict(scaled_x_test)\n\nacc = accuracy_score(y_test, pred_test)\n# recall = precision_score(y_train, pred_train)\nprint(acc)\nclass_rep = classification_report(y_test, pred_test, digits=3)\nprint(class_rep)","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# date range \nimport pandas as pd \n\ndf =pd.date_range(\"2000-01-01\",\"2020-12-01\", freq='MS')\ndf","metadata":{"execution":{"iopub.execute_input":"2024-04-15T15:03:31.532955Z","iopub.status.busy":"2024-04-15T15:03:31.532457Z","iopub.status.idle":"2024-04-15T15:03:31.551041Z","shell.execute_reply":"2024-04-15T15:03:31.549699Z","shell.execute_reply.started":"2024-04-15T15:03:31.532919Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## 불군형 처리 \nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_classification\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\n\nx, y = make_classification(n_samples=2000, n_features=5, weights=[0.3], flip_y=0, n_classes=2)\n# x, y = make_classification(n_samples=2000,n_features=5,n_informative=3, n_clusters_per_class=1, n_classes=3,weights=[0.5,0.2])\n\nprint(x.shape, y.shape)\n# x = pd.DataFrame(x)\n# y = pd.DataFrame(y)\ns = Counter(y)\nprint(type(s),s) \n\n# print(x.info(),y.aligninfo())\n\n# import matplotlib.pyplot as plt\n\n# fig,ax = plt.subplots(5,1)\n\n# ax[0].scatter(y, x[0])\n# ax[1].scatter(y, x[1])\n# ax[2].scatter(y, x[2])\n# ax[3].scatter(y, x[3])\n# ax[4].scatter(y, x[4])\n# plt.show()\n\n# help(make_classification)","metadata":{"execution":{"iopub.execute_input":"2024-04-15T15:01:33.787027Z","iopub.status.busy":"2024-04-15T15:01:33.786499Z","iopub.status.idle":"2024-04-15T15:01:33.802408Z","shell.execute_reply":"2024-04-15T15:01:33.800979Z","shell.execute_reply.started":"2024-04-15T15:01:33.786985Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#언더샘플링\n# undersample = RandomUnderSampler(sampling_strategy='majority')\nundersample = RandomUnderSampler(sampling_strategy='auto')\nx_under, y_under = undersample.fit_resample(x, y)\nprint(Counter(y_under))\n# undersample = RandomUnderSampler(sampling_strategy=0.5) \n# x_under2, y_under2 = undersample.fit_resample(x, y)\n# print(Counter(y_under2))","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:58:53.369311Z","iopub.status.busy":"2024-04-15T14:58:53.368804Z","iopub.status.idle":"2024-04-15T14:58:53.389599Z","shell.execute_reply":"2024-04-15T14:58:53.388171Z","shell.execute_reply.started":"2024-04-15T14:58:53.369275Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 오버 샘플링\nfrom imblearn.over_sampling import RandomOverSampler\n# oversample = RandomOverSampler(sampling_strategy=0.5) \noversample = RandomOverSampler(sampling_strategy='auto')\nx_over, y_over = oversample.fit_resample(x, y)\n\nprint(Counter(y_over))\n\nhelp(oversample.fit_resample)","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:59:52.362011Z","iopub.status.busy":"2024-04-15T14:59:52.361463Z","iopub.status.idle":"2024-04-15T14:59:52.381712Z","shell.execute_reply":"2024-04-15T14:59:52.380450Z","shell.execute_reply.started":"2024-04-15T14:59:52.361970Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#SMOTE\nfrom imblearn.over_sampling import SMOTE\nsmote_sample = SMOTE(sampling_strategy='minority') \nx_sm, y_sm = smote_sample.fit_resample(x, y)\nprint(Counter(y_sm))","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:53:31.779479Z","iopub.status.busy":"2024-04-15T14:53:31.778469Z","iopub.status.idle":"2024-04-15T14:53:31.796592Z","shell.execute_reply":"2024-04-15T14:53:31.795010Z","shell.execute_reply.started":"2024-04-15T14:53:31.779404Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **6 머신러닝**","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:04:48.562867Z","iopub.status.busy":"2024-04-15T14:04:48.560967Z","iopub.status.idle":"2024-04-15T14:04:48.763466Z","shell.execute_reply":"2024-04-15T14:04:48.762028Z","shell.execute_reply.started":"2024-04-15T14:04:48.562800Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import sklearn.linear_model as lm\n\n# print(dir(lm))\n# print(help(lm.QuantileRegressor))\nlms = {\"ARDRegression\":\"Bayesian ARD regression\",\"BayesianRidge\":\"Bayesian ridge regression\",\n       \"ElasticNet\":\"Linear regression with combined L1 and L2 priors as regularizer\",\n  \"ElasticNetCV\":\"Elastic Net model with iterative fitting along a regularization path\",\n  \"GammaRegressor\":\"Generalized Linear Model with a Gamma distribution\",\n  \"HuberRegressor\":\"L2-regularized linear regression model that is robust to outliers\",\n  \"Lasso\":\"Linear Model trained with L1 prior as regularizer\", \n  \"PassiveAggressiveClassifier\":\"Passive Aggressive Classifier\",\n       \"PoissonRegressor\":\"Generalized Linear Model with a Poisson distribution\",\n  \"SGDRegressor\":\"Linear model fitted by minimizing a regularized empirical loss with SGD\",\n\"QuantileRegressor\":\"Linear regression model that predicts conditional quantiles\",\n\"RANSACRegressor\":\"이상치에 강건한 회귀모델\", \n\"TheilSenRegressor\":\"이상치에 강건한 회귀모델, Theil-Sen 추정량(Theil-Sen Estimator)은 중앙값을 사용하여 추정되는 선형 회귀 모델입니다\",\n\"TweedieRegressor\":\"Tweedie 분포를 기반으로 한 회귀 모델\", \"ridge_regression\":\"ridge regression\"\n}\nprint(lms)\n","metadata":{"execution":{"iopub.execute_input":"2024-04-15T13:58:54.956518Z","iopub.status.busy":"2024-04-15T13:58:54.956074Z","iopub.status.idle":"2024-04-15T13:58:54.966220Z","shell.execute_reply":"2024-04-15T13:58:54.964937Z","shell.execute_reply.started":"2024-04-15T13:58:54.956486Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 통계 파트 - 검정\n\n- 독립변수가 범주형이고, 종속변수가 연속형이면 t 검정, anova (분산분석) 검증을 수행함\n  . 정규성, 등분산, ttest_1samp, ttest_ind, ttest_rel, wilcoxn, kruskal, pg.welch_anova, stats.f_oneway, anova_lm\n- 독집변수가 범주형이고, 종속변수가 범주형이면 카이제곱 검정을 수행함 \n  . 적합성, 독립성, 동질성\n","metadata":{}},{"cell_type":"markdown","source":"## **7.1 통계처리**","metadata":{}},{"cell_type":"code","source":"# 통계 처리 \n\nimport scipy\nimport scipy.stats as st \n\nfrom scipy.stats import bernoulli, norm, binom, nbinom, geom, hypergeom,multinomial, t, f\n\nnorm(10,2).pdf(11)","metadata":{"execution":{"iopub.execute_input":"2024-03-02T12:34:28.587371Z","iopub.status.busy":"2024-03-02T12:34:28.586880Z","iopub.status.idle":"2024-03-02T12:34:28.601236Z","shell.execute_reply":"2024-03-02T12:34:28.599754Z","shell.execute_reply.started":"2024-03-02T12:34:28.587336Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.2 t-test**","metadata":{}},{"cell_type":"code","source":"# t-test \n# 정규성, 등분산 검정 \nfrom scipy.stats import shapiro, levene, bartlett \nfrom scipy.stats import ttest_1samp, ttest_rel, ttest_ind\n\n# help(sklearn)\na = [3,3.5,2, 1.8, 1]\nb = st.norm(3,0.8).rvs(100)\n# print(help(st.norm))\nprint(np.mean(a), np.std(a)) \n# 정규성 검정 \nr = st.shapiro(a)\nr2 = st.shapiro(b)\nprint(r,r2)\n#  정규성 만족-1samp T\nret =  st.ttest_1samp(a,2.0, alternative='less')\n#  정규성 불만족 - wilcoxon\nret = st.wilcoxon(a)\n# ret = st.wilcoxon(a,a)\n#  등분산 검정 \nr_eqv = st.levene(b, a)\nr_eqv2 = st.bartlett(b, a)\nprint(r_eqv,r_eqv2)\n# 2samp -ind T\nret =  st.ttest_ind(a,b, equal_var=True )\nret2 = st.ttest_ind(a,b, equal_var=False)\nprint(ret)\n# 2samp - ind\nret3 = st.ttest_rel(a,a)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.3 anova - 1원배치 분산분석**","metadata":{}},{"cell_type":"code","source":"# anova 1원 배치 \n# stats.kruskal(setosa,versicolor,virginica)\n#정규성 검사 \nfrom scipy.stats import shapiro\n\n# 정규성 만족하지 않으면 \nfrom scipy.stats import kruskal \n#stats.kruskal(setosa,versicolor,virginica)\n\n#정규성 만족하면, 등분산 검사 \nfrom scipy.stats import bartlett,levene\n\n# stats.levene(setosa,versicolor,virginica)\n# 등분산을 만족하면 \nfrom scipy.stats import f_oneway\n# stats.f_oneway(setosa,versicolor,virginica)\n# 등분산을 만족하지 않으면 \nimport pingouin as pg \n#pg.welch_anova(data = Iris_data, dv ='sepal width', between='target')\n# 사후검정 \nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.stats.multicomp import MultiComparison\n# mc = MultiComparison(data= Iris_data[\"sepal width\"], groups=Iris_data[\"target\"] )\n# tuekeyhsd = mc.tukeyhsd(alpha=0.05)\n# fig = tuekeyhsd.plot_simultaneous()\n#tuekeyhsd.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.3 anova - 2원배치 분산 분석**","metadata":{}},{"cell_type":"code","source":"# anova 2원 배치 \n## 분산분석 수행 \nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n# formula = 'mpg ~ C(cyl) + C(am) + C(cyl):C(am)'\n# model = ols(formula, mtcars).fit()\n# aov_table = anova_lm(model, typ=3)\nprint(dir(anova_lm))\n\nfrom statsmodels.graphics.factorplots import interaction_plot\n#import matplotlib.pyplot as plt\n## 독립변수 cyl,am와 종속변수 mpg을 Series로 변경 \n# cyl = mtcars[\"cyl\"]\n# am = mtcars[\"am\"]\n# mpg = mtcars[\"mpg\"]\n# fig, ax = plt.subplots(figsize=(6, 6))\n# fig = interaction_plot(cyl,am, mpg,\n#                        colors=['red', 'blue'], markers=['D', '^'], ms=10, ax=ax)","metadata":{"execution":{"iopub.execute_input":"2024-04-15T14:03:47.419457Z","iopub.status.busy":"2024-04-15T14:03:47.419003Z","iopub.status.idle":"2024-04-15T14:03:47.878079Z","shell.execute_reply":"2024-04-15T14:03:47.875243Z","shell.execute_reply.started":"2024-04-15T14:03:47.419425Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.4 교차분석**\n","metadata":{}},{"cell_type":"code","source":"#교차분석 \n#적합도 거정 \n# 유의수준 0.05로 적합도 검정 수행\nfrom scipy.stats import chisquare\n# chi = chisquare(table, f_exp=[171,171])\n# print('<적합도 검정>\\n',chi)\n\n# 카이제곱 검정을 통한 독립성 검정 수행\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n# 데이터 불러오기\n# df = pd.read_csv(\"/kaggle/input/adp-realdataset/titanic.csv\")\ndf = pd.read_csv(\"./data/titanic.csv\")\nprint(df.info())\nprint(df[\"embark_town\".strip()].unique())\n\nprint(df.describe(include='all'))\nprint(df.columns, df.isna().sum(),  df[\"embark_town\".strip()].value_counts())\n\ntotal = len(df)\nage = df[\"age\"].isna().sum()\nprint(age/total*100)\n\ndf[\"age\"] = df.fillna(df[\"age\"].mean())\n\ntitanic = df\n\nprint(titanic.isna().sum())\n\ntable = pd.crosstab(titanic['class'], titanic['survived'])\nchi, p, df, expect = chi2_contingency(table) \nprint('Statistic:', chi)\nprint('p-value:', p)\nprint('df:', df)\nprint('expect: \\n', expect)\n\ntitanic.info()","metadata":{"execution":{"iopub.execute_input":"2024-04-16T00:00:37.583326Z","iopub.status.busy":"2024-04-16T00:00:37.582893Z","iopub.status.idle":"2024-04-16T00:00:40.225453Z","shell.execute_reply":"2024-04-16T00:00:40.224096Z","shell.execute_reply.started":"2024-04-16T00:00:37.583291Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.5 회귀분석**","metadata":{}},{"cell_type":"code","source":"from statsmodels.formula.api import ols\n# y = house['price']\n# X = house[['sqft_living']]\n# # 단순선형회귀모형 적합\n# lr = ols('price ~ sqft_living',data=house).fit()\n# y_pred = lr.predict(X)\n# lr.summary()\n\nfrom patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# 독립변수와 종속변수를 데이터프레임으로 나누어 저장하는 함수 \n# y,X = dmatrices(\"Price ~ EngineSize + RPM + Weight+ Length + MPGcity + MPGhighway\",\n#                 data = Cars,return_type =\"dataframe\")\n# # 독립변수끼리의 VIF값을 계산하여 데이터프레임으로 만드는 과정 \n# vif_list = []\n# for i in range(1,len(X.columns)): \n#     vif_list.append([variance_inflation_factor(X.values,i), X.columns[i]])\n# pd.DataFrame(vif_list,columns=['vif','variable'])\n# model = smf.ols(formula =\"Price ~ EngineSize + RPM + Weight  + MPGhighway\", data = Cars)\n# result = model.fit()\n# result.summary()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 전진 선택법 \nimport time\nimport itertools\ndef processSubset(X,y, feature_set):\n            model = sm.OLS(y,X[list(feature_set)]) # Modeling\n            regr = model.fit() # 모델 학습\n            AIC = regr.aic # 모델의 AIC\n            return {\"model\":regr, \"AIC\":AIC}\n# 전진선택법\ndef forward(X, y, predictors):\n    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n    remaining_predictors = [p for p in X.columns.difference(['Intercept']) if p not in predictors]\n    results = []\n    for p in remaining_predictors:\n        results.append(processSubset(X=X, y= y,            feature_set=predictors+[p]+['Intercept']))\n        \n    # 데이터프레임으로 변환\n    models = pd.DataFrame(results)\n    # AIC가 가장 낮은 것을 선택\n    best_model = models.loc[models['AIC'].argmin()] # index\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\")\n    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n    return best_model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 후진소거법\ndef backward(X,y,predictors):\n    tic = time.time()\n    results = []\n    \n    # 데이터 변수들이 미리정의된 predictors 조합 확인\n    for combo in itertools.combinations(predictors, len(predictors) -1):\n        results.append(processSubset(X=X, y= y,        feature_set = list(combo)+['Intercept']))\n    models = pd.DataFrame(results)\n    \n    # 가장 낮은 AIC를 가진 모델을 선택\n    best_model = models.loc[models['AIC'].argmin()]\n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\",          len(predictors) -1, \"predictors in\", (toc - tic))\n    print('Selected predictors:',best_model['model'].model.exog_names,         'AIC:',best_model[0] )\n\n    return best_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 단계적 선택법\ndef Stepwise_model(X,y):\n    Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n    tic = time.time()\n    predictors = []\n    Smodel_before = processSubset(X,y,predictors+['Intercept'])['AIC']\n\n    for i in range(1, len(X.columns.difference(['Intercept'])) +1):\n        Forward_result = forward(X=X, y=y, predictors=predictors) \n        print('forward')\n        Stepmodels.loc[i] = Forward_result\n        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n        predictors = [ k for k in predictors if k !='Intercept']\n        Backward_result = backward(X=X, y=y, predictors=predictors)\n\n        if Backward_result['AIC']< Forward_result['AIC']:\n            Stepmodels.loc[i] = Backward_result\n            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n            predictors = [ k for k in predictors if k !='Intercept']\n            print('backward')\n\n        if Stepmodels.loc[i]['AIC']> Smodel_before:\n            break\n        else:\n            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n    toc = time.time()\n    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n\n    return (Stepmodels['model'][len(Stepmodels['model'])])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Stepwise_best_model = Stepwise_model(X=X, y=y)\nStepwise_best_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.6 군집분석** - Dendrogram, KMeans, GausianMixture, ","metadata":{}},{"cell_type":"code","source":"## 군집분석 \nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import calinski_harabasz_score\n# calinsk_harabasz_score\n# for k in range(2, 10):\n#     kmeans_model = KMeans(n_clusters=k, random_state=1).fit(X)\n#     labels = kmeans_model.labels_\n#     print(calinski_harabasz_score(X, labels))\n# elbow\n# for i in range(1, 11):\n#     km=KMeans(n_clusters=i, random_state=1)\n#     km.fit(X)\n#     sse.append(km.inertia_) # km.inertia_\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture \n# iris = pd.read_csv(\"../data/iris.csv\")\n# iris.info()\n# df = iris.drop(\"target\", axis=1)\n# scaler = StandardScaler()\n# df_scaled = scaler.fit_transform(df)\n\n# gmm = GaussianMixture(n_components=3)\n# gmm.fit(df_scaled)\n# gmm_labels = gmm.predict(df_scaled)\n# gmm_labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# statsmodels.formula.api vs statsmodels.api 의 차이는 \n 전자는 자동으로 상수항을 추가해 준다면, 후자는 x_new = sm.add_constant(x) 과 같이 처리해주고 SM_model_2 = sm.OLS(y, x_new).fit() 이렇게 해주어야 한다. \n https://michael-fuchs-python.netlify.app/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/","metadata":{}},{"cell_type":"markdown","source":"연관 분석(association analysis)에서 \"lift(향상도)\"는 상품 간의 연관성을 측정하는 데 사용되는 지표 중 하나입니다. \n<span style=\"color:red\"> 향상도는 두 개의 상품 간의 관계가 랜덤하게 발생한 경우에 비해 그 빈도가 얼마나 높은지를 측정합니다.</span>\n일반적으로 향상도는 다음과 같은 공식으로 계산됩니다:\n향상도의 해석은 다음과 같습니다:\n향상도가 1보다 크면, 두 상품 간의 연관 관계가 랜덤한 경우보다 더 자주 함께 발생한다는 것을 의미합니다. 이는 양의 상관 관계를 나타냅니다.\n향상도가 1에 가까우면, 두 상품 간의 연관성이 랜덤한 경우와 비슷하다는 것을 의미합니다.\n향상도가 1보다 작으면, 두 상품 간의 연관 관계가 랜덤한 경우보다 덜 자주 함께 발생한다는 것을 의미합니다. 이는 음의 상관 관계를 나타냅니다.\n따라서 향상도는 연관 규칙의 유용성을 평가하는 데 사용되며, 특정 상품이 다른 상품과 어떻게 관련되어 있는지를 파악하는 데 도움이 됩니다.","metadata":{}},{"cell_type":"markdown","source":"## **7.7 연관분석**","metadata":{}},{"cell_type":"code","source":"## 연관분석 \n#runstest_1samp, apriori, assocaiton_rules  \n# 연관성에 대한 검정 \nfrom statsmodels.sandbox.stats.runs import runstest_1samp\n# 연관규칙분석 \n#  Lift 해석은 1보다 크며, 연관이 \nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","metadata":{"execution":{"iopub.execute_input":"2024-03-02T06:49:23.109848Z","iopub.status.busy":"2024-03-02T06:49:23.109501Z","iopub.status.idle":"2024-03-02T06:49:23.114605Z","shell.execute_reply":"2024-03-02T06:49:23.113290Z","shell.execute_reply.started":"2024-03-02T06:49:23.109821Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \n\ndf =pd.DataFrame(np.random.randint(100,1000,100))\nss = [[\"a\",\"b\",\"c\"],[\"d\",\"e\"],[\"a\",\"b\",\"c\",\"d\",\"e\"]]\n\nfrom  mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_ary = te.fit_transform(ss)\n# print(help(te))\nprint(type(te), te.columns_, te_ary)\nprint(te.inverse_transform(te_ary))\n\ndf = pd.DataFrame(te_ary, columns = te.columns_)\nprint(df)\n\n# df.plot()\n# plt.show()","metadata":{"execution":{"iopub.execute_input":"2024-03-02T07:30:48.754866Z","iopub.status.busy":"2024-03-02T07:30:48.754291Z","iopub.status.idle":"2024-03-02T07:30:48.763905Z","shell.execute_reply":"2024-03-02T07:30:48.762949Z","shell.execute_reply.started":"2024-03-02T07:30:48.754843Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.8 시계열 분석**","metadata":{}},{"cell_type":"code","source":"# 7-8.시계열, 이원분석 anova, lm_anova, tsa,\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\ndata = pd.read_csv(\"./data/arima_data.csv\", names=['day','price'])\ndata.head()\n# data.columns = [\"day\",\"value\"]\ndata[\"day\"] = pd.to_datetime(data[\"day\"],format=\"%Y-%m-%d\")\ndata.set_index([\"day\"], inplace=True)\nprint(data.info(), data.head())\n\nts = data\nresult = seasonal_decompose(ts, model='multiplicative')\nplt.rcParams['figure.figsize'] = [12, 8]\nresult.plot()\nplt.show()\n\n## 정상성 \nfrom statsmodels.tsa.stattools import adfuller\ntraining = data[:\"2016-12-01\"]\ntest = data.drop(training.index)\nprint(help(adfuller))\n\n# ARIMA\nadf = adfuller(training, regression='ct') # c : constant, t : trend , n : no, ctt : linear and quadratic trend  \nprint(adf)\nfrom statsmodels.api import PCA, OLS, MANOVA\nfrom statsmodels.formula.api import ols \nfrom statsmodels.tsa import api \nfrom statsmodels.tsa.arima.model import ARIMA\n# PACF, ACF\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.arima.model import ARIMA\nmodel = ARIMA(training, order=(2,1,2))\nres = model.fit()\nres.summary()\nplt.plot(res.predict())\nplt.plot(training)\nplt.legend([\"P\",\"R\"])\nplt.show()\n","metadata":{"execution":{"iopub.execute_input":"2024-04-24T11:53:07.298031Z","iopub.status.busy":"2024-04-24T11:53:07.297658Z","iopub.status.idle":"2024-04-24T11:53:07.334498Z","shell.execute_reply":"2024-04-24T11:53:07.333098Z","shell.execute_reply.started":"2024-04-24T11:53:07.298001Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"td = data.index[-1] - data.index[0]\nprint(td.seconds) \nfrom datetime import timedelta\nhelp(timedelta)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 7-8.\nfrom sklearn.model_selection import train_test_split\n\ndata = pd.read_csv(\"./data/arima_data.csv\")\ndata.columns = [\"day\",\"value\"]\ndata[\"day\"] = pd.to_datetime(df[\"day\"],format=\"%Y-%m-%d\")\ndata.set_index([\"day\"], inplace=True)\nprint(data.info(), data.head())\ntraining = data[:\"2016-12-01\"]\ntest = data.drop(training.index)","metadata":{"execution":{"iopub.execute_input":"2024-04-24T12:24:14.709568Z","iopub.status.busy":"2024-04-24T12:24:14.708356Z","iopub.status.idle":"2024-04-24T12:24:14.794449Z","shell.execute_reply":"2024-04-24T12:24:14.792542Z","shell.execute_reply.started":"2024-04-24T12:24:14.709517Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pmdarima import auto_arima\nauto_model = auto_arima(training, start_p=0, d=1, start_q=0,\n                        max_p=3, max_q=3, \n                        start_P=0, start_Q=0,\n                        max_P=3, max_Q=3, m=12,\n                        seasonal=True, information_criterion='aic',\n                        trace=True)\n# m =12면 달 기준, m  =1 이면 년 단위 \nauto_model.summary()","metadata":{"execution":{"iopub.execute_input":"2024-04-24T12:24:14.709568Z","iopub.status.busy":"2024-04-24T12:24:14.708356Z","iopub.status.idle":"2024-04-24T12:24:14.794449Z","shell.execute_reply":"2024-04-24T12:24:14.792542Z","shell.execute_reply.started":"2024-04-24T12:24:14.709517Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"auto_pred_y= pd.DataFrame(auto_model.predict(n_periods=len(test)), \n                          index=test.index) \nauto_pred_y.columns = ['predicted_price']\nauto_pred_y","metadata":{"execution":{"iopub.execute_input":"2024-04-24T12:24:14.709568Z","iopub.status.busy":"2024-04-24T12:24:14.708356Z","iopub.status.idle":"2024-04-24T12:24:14.794449Z","shell.execute_reply":"2024-04-24T12:24:14.792542Z","shell.execute_reply.started":"2024-04-24T12:24:14.709517Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint(\"r2_score : \", r2_score(test, auto_pred_y))\nRMSE = mean_squared_error(test, auto_pred_y)**0.5\nprint(\"RMSE : \" , RMSE)","metadata":{"execution":{"iopub.execute_input":"2024-04-24T12:24:14.709568Z","iopub.status.busy":"2024-04-24T12:24:14.708356Z","iopub.status.idle":"2024-04-24T12:24:14.794449Z","shell.execute_reply":"2024-04-24T12:24:14.792542Z","shell.execute_reply.started":"2024-04-24T12:24:14.709517Z"}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### GrdidsearchCV 사용법 \n- GridSearchCV를 임포트 하고 \n- 모델을 선정하고, 모델의 파라미터를 정의해서 넣어준다. \n- 사용법은 fit, predict 를 사용하며 \n- 최적의 모델은 model.best_params_ 를 통하여 확인한다. \n- 각 모델별로 파라미터에 대한 이해가 있어야 한다. ","metadata":{"execution":{"iopub.execute_input":"2024-04-24T12:24:33.380521Z","iopub.status.busy":"2024-04-24T12:24:33.380056Z","iopub.status.idle":"2024-04-24T12:24:33.415528Z","shell.execute_reply":"2024-04-24T12:24:33.414014Z","shell.execute_reply.started":"2024-04-24T12:24:33.380484Z"}}},{"cell_type":"code","source":"# GridserachCV 사용하여 파라미터를 변경해가면서 최적의 모델을 찾기 \n# LinearRegression, LightGBM, RandomForestRegressor 등의 파라미터 자동 세팅 \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.cluster import DBSCAN, k_means, KMeans\nfrom sklearn.svm import SVC, SVR\n\nhelp(SVR)\nparam = {\"C\":[0.1, 1, 10, 100],  \"gamma\" : [0.001, 0.01, 0.1, 1, 10] }\n\ngrid_svm = GridSearchCV(SVR(), param_grid=param, cv = 5)\ngrid_svm.fit(x_trained_scaled, y_train)\nresult = pd.DataFrame(grid_svm.cv_results_[\"params\"])\nresult[\"mean_test_score\"] = grid_svm.cv_results_[\"mean_test_score\"]\nresult.sort_values(\"mean_test_score\", ascending=False)\ngrid_svm.best_params_\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model fit 하고 시간 구하기 \n- 앞선 예제에서 확인 4.5","metadata":{}},{"cell_type":"code","source":"# model fit 하기 \nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nll = os.listdir(\".\\data\")\n# print(ll)\ndf = pd.read_csv(\".\\\\data\\\\mtcars.csv\")\nprint(df.info(), df.head())\ndf.rename(columns={'Unnamed: 0':'name'}, inplace=True)\ndf = df[df.columns.difference([\"name\"])]\nprint(df.describe(include='all'))\n\nfig, ax = plt.subplots(1,1, figsize=(10,5))\n\nsns.boxplot( data = df, ax = ax)\nplt.show()\n\ny = df[\"mpg\"]\nx = df[df.columns.difference([\"mpg\"])]\n# clms = df.columns.difference([\"target\"])\n                             \nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3 )\n\nreg = LinearRegression()\nreg.fit(x_train, y_train)\n\npred_y = reg.predict(x_test)\nr2 = reg.score(x_test, y_test)\nr2\n\n# print(df.head())\n# def test_model(model, train_x, train_y, test_x, test_y):\n#     model.fit(train_x, train_y)\n#     model.predict(test_x)\n\n# from sklearn.linear_model import LinearRegression\n\n# reg = LinearRegression()\n# reg.fit(x, y)\n\n\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mtcars = pd.read_csv(\"./data/mtcars.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"date\"] = ['2021-01-01']","metadata":{},"execution_count":null,"outputs":[]}]}