{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30664,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"raw","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# from unittest import skip\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport sklearn \nimport scipy.stats as st\n    \n# help(sklearn)\n\na = [3,3.5,2, 1.8, 1]\nb = st.norm(3,0.8).rvs(100)\n# print(help(st.norm))\nprint(np.mean(a), np.std(a))\nr = st.shapiro(a)\nr2 = st.shapiro(b)\nprint(r,r2)\nret =  st.ttest_1samp(a,2.0, alternative='less')\nr_eqv = st.levene(b, a)\nr_eqv2 = st.bartlett(b, a)\nprint(r_eqv,r_eqv2)\nret =  st.ttest_ind(a,b, equal_var=True )\nprint(ret)\n ","metadata":{"execution":{"iopub.status.busy":"2024-03-09T05:43:29.174868Z","iopub.execute_input":"2024-03-09T05:43:29.175230Z","iopub.status.idle":"2024-03-09T05:43:29.188233Z","shell.execute_reply.started":"2024-03-09T05:43:29.175204Z","shell.execute_reply":"2024-03-09T05:43:29.187131Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"2.2600000000000002 0.8890444308357147\nShapiroResult(statistic=0.963385283946991, pvalue=0.8313377499580383) ShapiroResult(statistic=0.9833040237426758, pvalue=0.23821096122264862)\nLeveneResult(statistic=0.08895495868488283, pvalue=0.7661107657004158) BartlettResult(statistic=0.23501151808713103, pvalue=0.6278325065581375)\nTtestResult(statistic=-2.18277445032824, pvalue=0.03132390023802373, df=103.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import scipy\nfrom statsmodels.tsa import arima,api \n\nll = [i for i in dir(scipy.stats) if \"ba\" in i]\nll\n","metadata":{"execution":{"iopub.status.busy":"2024-03-02T12:03:06.958039Z","iopub.execute_input":"2024-03-02T12:03:06.958534Z","iopub.status.idle":"2024-03-02T12:03:06.967327Z","shell.execute_reply.started":"2024-03-02T12:03:06.958497Z","shell.execute_reply":"2024-03-02T12:03:06.966305Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"['_mstats_basic',\n 'barnard_exact',\n 'bartlett',\n 'bayes_mvs',\n 'crystalball',\n 'mstats_basic']"},"metadata":{}}]},{"cell_type":"markdown","source":"## **4 전처리** ","metadata":{}},{"cell_type":"code","source":"# 기본적인 모듈, EDA시 \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn \nimport statsmodels.tsa\n\n# 전처리 \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.decomposition import PCA\n# pca = PCA( n_components = 4) \n# pca_fit = pca.fit(x)\n# print(\"고유 값 : \", pca.singular_values_)\n# print(\"분산 설명력: \", pca.explained_variance_ratio_)\n\nll = [i for i in dir(sklearn.metrics) if \"error\" in i ]\nstats = [i for i in dir(statsmodels.tsa)]\n# \n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:08:50.084702Z","iopub.execute_input":"2024-04-15T14:08:50.085231Z","iopub.status.idle":"2024-04-15T14:08:50.093782Z","shell.execute_reply.started":"2024-04-15T14:08:50.085192Z","shell.execute_reply":"2024-04-15T14:08:50.092732Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"## 불군형 처리 \nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_classification\nfrom collections import Counter\nfrom imblearn.under_sampling import RandomUnderSampler\n\nx, y = make_classification(n_samples=2000, n_features=5, weights=[0.3], flip_y=0, n_classes=2)\n# x, y = make_classification(n_samples=2000,n_features=5,n_informative=3, n_clusters_per_class=1, n_classes=3,weights=[0.5,0.2])\n\n# x = pd.DataFrame(x)\n# y = pd.DataFrame(y)\ns = Counter(y)\nprint(type(s),s) \n\n# print(x.head())\n\n# import matplotlib.pyplot as plt\n\n# fig,ax = plt.subplots(5,1)\n\n# ax[0].scatter(y, x[0])\n# ax[1].scatter(y, x[1])\n# ax[2].scatter(y, x[2])\n# ax[3].scatter(y, x[3])\n# ax[4].scatter(y, x[4])\n# plt.show()\n\n# help(make_classification)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:42:13.519257Z","iopub.execute_input":"2024-04-15T14:42:13.519736Z","iopub.status.idle":"2024-04-15T14:42:13.531287Z","shell.execute_reply.started":"2024-04-15T14:42:13.519704Z","shell.execute_reply":"2024-04-15T14:42:13.530411Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"<class 'collections.Counter'> Counter({1: 1400, 0: 600})\n","output_type":"stream"}]},{"cell_type":"code","source":"#언더샘플링\n# undersample = RandomUnderSampler(sampling_strategy='majority')\nundersample = RandomUnderSampler(sampling_strategy='auto')\nx_under, y_under = undersample.fit_resample(x, y)\nprint(Counter(y_under))\n# undersample = RandomUnderSampler(sampling_strategy=0.5) \n# x_under2, y_under2 = undersample.fit_resample(x, y)\n# print(Counter(y_under2))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:51:34.936807Z","iopub.execute_input":"2024-04-15T14:51:34.937417Z","iopub.status.idle":"2024-04-15T14:51:34.947338Z","shell.execute_reply.started":"2024-04-15T14:51:34.937375Z","shell.execute_reply":"2024-04-15T14:51:34.946077Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"Counter({0: 600, 1: 600})\n","output_type":"stream"}]},{"cell_type":"code","source":"# 오버 샘플링\nfrom imblearn.over_sampling import RandomOverSampler\n# oversample = RandomOverSampler(sampling_strategy=0.5) \noversample = RandomOverSampler(sampling_strategy='auto')\nx_over, y_over = oversample.fit_resample(x, y)\n\nprint(Counter(y_over))\n\n# help(RandomOverSampler)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:53:20.522867Z","iopub.execute_input":"2024-04-15T14:53:20.523320Z","iopub.status.idle":"2024-04-15T14:53:20.534653Z","shell.execute_reply.started":"2024-04-15T14:53:20.523289Z","shell.execute_reply":"2024-04-15T14:53:20.533149Z"},"trusted":true},"execution_count":153,"outputs":[{"name":"stdout","text":"Counter({1: 1400, 0: 1400})\n","output_type":"stream"}]},{"cell_type":"code","source":"#SMOTE\nfrom imblearn.over_sampling import SMOTE\nsmote_sample = SMOTE(sampling_strategy='minority') \nx_sm, y_sm = smote_sample.fit_resample(x, y)\nprint(Counter(y_sm))","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:53:31.778469Z","iopub.execute_input":"2024-04-15T14:53:31.779479Z","iopub.status.idle":"2024-04-15T14:53:31.796592Z","shell.execute_reply.started":"2024-04-15T14:53:31.779404Z","shell.execute_reply":"2024-04-15T14:53:31.795010Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stdout","text":"Counter({1: 1400, 0: 1400})\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **6 머신러닝**","metadata":{}},{"cell_type":"code","source":"# 머신러닝 - 회귀분석 \nfrom sklearn.linear_model import LinearRegression, LogisticRegression\n\nfrom sklearn.linear_model import SGDRegressor\n# 다항회귀\nfrom sklearn.preprocessing import PolynomialFeatures\n# poly_reg=PolynomialFeatures(degree=2)\n# X_poly=poly_reg.fit_transform(X_train.reshape(-1,1))\n\n# 리지, 라소, 엘라스틱 \nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet\n\n# 소프트맥스 회귀 \nfrom sklearn.linear_model import LogisticRegression\n# softm=LogisticRegression(multi_class='multinomial', solver='lbfgs', C=10)\n# softm.fit(train_x, train_y)\n\n# 머시너링 - 앙상블 \n ## 랜덤포레스트 \nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\n# clf = BaggingClassifier(base_estimator =DecisionTreeClassifier())\n# pred = clf.fit(x_train, y_train).predict(x_test)\n# print(\"Accuracy Score : \", clf.score(x_test, pred))\n\n## 배깅\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n# reg = BaggingRegressor(base_estimator =DecisionTreeRegressor(), oob_score=True)\n# reg=reg.fit(X, y)\n# reg.oob_score_\n\n## 부스팅 \nfrom sklearn.ensemble import AdaBoostClassifier\n# clf = AdaBoostClassifier(base_estimator =None)\n# pred=clf.fit(x_train, y_train).predict(x_test)\n# print(\"정확도 : \", clf.score(x_test, y_test))\nfrom sklearn.ensemble import AdaBoostRegressor\n# reg = AdaBoostRegressor(base_estimator =None)\n# pred=reg.fit(x_train, y_train).predict(x_test)\n\n# 머신러닝 - SVM \nfrom sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n# clf = SVC(C=0.5)\n# clf.fit(train_x, train_y)\n\n# KNN - 분류기\nfrom sklearn.neighbors import KNeighborsClassifier\n# KNN - 회귀식\nfrom sklearn.neighbors import KNeighborsRegressor\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n# clf=DecisionTreeClassifier(max_depth =5)\n# clf.fit(x_train, y_train)\nfrom sklearn.tree import export_graphviz\n# dt_dot_data = export_graphviz(clf, feature_names = feature_names,\n#                               class_names = target_names,\n#                               filled=True, rounded =True,\n#                               special_characters=True)\n# dt_graph=pydotplus.graph_from_dot_data(dt_dot_data)\n\n# GaussianNB\nfrom sklearn.naive_bayes import GaussianNB\n# gnb = GaussianNB()\n# pred = gnb.fit(x_train, y_train).predict(x_test)\n# print(\"Accuracy Score : \", gnb.score(x_test, y_test))\n\n#베르누이 \nfrom sklearn.feature_extraction.text import CountVectorizer \n# cv = CountVectorizer(binary =True)\n# x_traincv = cv.fit_transform(x_train)\n# x_traincv.shape\nfrom sklearn.naive_bayes import BernoulliNB\n# bnb = BernoulliNB()\n# bnb.fit(x_traincv, y_train)\n# 머신러닝 - 평가 지표 \n\n# 다항 나이브베이즈 \nfrom sklearn.naive_bayes import MultinomialNB\n# mnb = MultinomialNB()\n# mnb.fit(x_traincv, y_train)\n\n# 평가지표 \nfrom sklearn.metrics import mean_absolute_error,mean_squared_error\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:04:48.560967Z","iopub.execute_input":"2024-04-15T14:04:48.562867Z","iopub.status.idle":"2024-04-15T14:04:48.763466Z","shell.execute_reply.started":"2024-04-15T14:04:48.562800Z","shell.execute_reply":"2024-04-15T14:04:48.762028Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import sklearn.linear_model as lm\n\n# print(dir(lm))\n\n# print(help(lm.QuantileRegressor))\nlms = {\"ARDRegression\":\"Bayesian ARD regression\",\"BayesianRidge\":\"Bayesian ridge regression\",\n       \"ElasticNet\":\"Linear regression with combined L1 and L2 priors as regularizer\",\n  \"ElasticNetCV\":\"Elastic Net model with iterative fitting along a regularization path\",\n  \"GammaRegressor\":\"Generalized Linear Model with a Gamma distribution\",\n  \"HuberRegressor\":\"L2-regularized linear regression model that is robust to outliers\",\n  \"Lasso\":\"Linear Model trained with L1 prior as regularizer\", \n  \"PassiveAggressiveClassifier\":\"Passive Aggressive Classifier\",\n       \"PoissonRegressor\":\"Generalized Linear Model with a Poisson distribution\",\n  \"SGDRegressor\":\"Linear model fitted by minimizing a regularized empirical loss with SGD\",\n\"QuantileRegressor\":\"Linear regression model that predicts conditional quantiles\",\n\"RANSACRegressor\":\"이상치에 강건한 회귀모델\", \n\"TheilSenRegressor\":\"이상치에 강건한 회귀모델, Theil-Sen 추정량(Theil-Sen Estimator)은 중앙값을 사용하여 추정되는 선형 회귀 모델입니다\",\n\"TweedieRegressor\":\"Tweedie 분포를 기반으로 한 회귀 모델\", \"ridge_regression\":\"ridge regression\"\n}\nprint(lms)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-15T13:58:54.956074Z","iopub.execute_input":"2024-04-15T13:58:54.956518Z","iopub.status.idle":"2024-04-15T13:58:54.966220Z","shell.execute_reply.started":"2024-04-15T13:58:54.956486Z","shell.execute_reply":"2024-04-15T13:58:54.964937Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"{'ARDRegression': 'Bayesian ARD regression', 'BayesianRidge': 'Bayesian ridge regression', 'ElasticNet': 'Linear regression with combined L1 and L2 priors as regularizer', 'ElasticNetCV': 'Elastic Net model with iterative fitting along a regularization path', 'GammaRegressor': 'Generalized Linear Model with a Gamma distribution', 'HuberRegressor': 'L2-regularized linear regression model that is robust to outliers', 'Lasso': 'Linear Model trained with L1 prior as regularizer', 'PassiveAggressiveClassifier': 'Passive Aggressive Classifier', 'PoissonRegressor': 'Generalized Linear Model with a Poisson distribution', 'SGDRegressor': 'Linear model fitted by minimizing a regularized empirical loss with SGD', 'QuantileRegressor': 'Linear regression model that predicts conditional quantiles', 'RANSACRegressor': '이상치에 강건한 회귀모델', 'TheilSenRegressor': '이상치에 강건한 회귀모델, Theil-Sen 추정량(Theil-Sen Estimator)은 중앙값을 사용하여 추정되는 선형 회귀 모델입니다', 'TweedieRegressor': 'Tweedie 분포를 기반으로 한 회귀 모델', 'ridge_regression': 'ridge regression'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.1 통계처리**","metadata":{}},{"cell_type":"code","source":"# 통계 처리 \n\nimport scipy\nimport scipy.stats as st \n\nfrom scipy.stats import bernoulli, norm, binom, nbinom, geom, hypergeom,multinomial\n\nnorm(10,2).pdf(11)","metadata":{"execution":{"iopub.status.busy":"2024-03-02T12:34:28.586880Z","iopub.execute_input":"2024-03-02T12:34:28.587371Z","iopub.status.idle":"2024-03-02T12:34:28.601236Z","shell.execute_reply.started":"2024-03-02T12:34:28.587336Z","shell.execute_reply":"2024-03-02T12:34:28.599754Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"0.17603266338214976"},"metadata":{}}]},{"cell_type":"markdown","source":"## **7.2 t-test**","metadata":{}},{"cell_type":"code","source":"# t-test \n# 정규성, 등분산 검정 \nfrom scipy.stats import shapiro, levene, bartlett \nfrom scipy.stats import ttest_1samp, ttest_rel, ttest_ind","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.3 anova - 1원배치 분산분석**","metadata":{}},{"cell_type":"code","source":"# anova 1원 배치 \n\n# stats.kruskal(setosa,versicolor,virginica)\n#정규성 검사 \nfrom scipy.stats import shapiro\n\n# 정규성 만족하지 않으면 \nfrom scipy.stats import kruskal \n#stats.kruskal(setosa,versicolor,virginica)\n\n#정규성 만족하면, 등분산 검사 \nfrom scipy.stats import bartlett,levene\n\n# stats.levene(setosa,versicolor,virginica)\n# 등분산을 만족하면 \nfrom scipy.stats import f_oneway\n# stats.f_oneway(setosa,versicolor,virginica)\n# 등분산을 만족하지 않으면 \nimport pingouin as pg \n#pg.welch_anova(data = Iris_data, dv ='sepal width', between='target')\n# 사후검정 \nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\nfrom statsmodels.stats.multicomp import MultiComparison\n# mc = MultiComparison(data= Iris_data[\"sepal width\"], groups=Iris_data[\"target\"] )\n# tuekeyhsd = mc.tukeyhsd(alpha=0.05)\n# fig = tuekeyhsd.plot_simultaneous()\n#tuekeyhsd.summary()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.3 anova - 2원배치 분산 분석**","metadata":{}},{"cell_type":"code","source":"# anova 2원 배치 \n## 분산분석 수행 \nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.anova import anova_lm\n# formula = 'mpg ~ C(cyl) + C(am) + C(cyl):C(am)'\n# model = ols(formula, mtcars).fit()\n# aov_table = anova_lm(model, typ=3)\nprint(dir(anova_lm))\n\nfrom statsmodels.graphics.factorplots import interaction_plot\n#import matplotlib.pyplot as plt\n## 독립변수 cyl,am와 종속변수 mpg을 Series로 변경 \n# cyl = mtcars[\"cyl\"]\n# am = mtcars[\"am\"]\n# mpg = mtcars[\"mpg\"]\n# fig, ax = plt.subplots(figsize=(6, 6))\n# fig = interaction_plot(cyl,am, mpg,\n#                        colors=['red', 'blue'], markers=['D', '^'], ms=10, ax=ax)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:03:47.419003Z","iopub.execute_input":"2024-04-15T14:03:47.419457Z","iopub.status.idle":"2024-04-15T14:03:47.878079Z","shell.execute_reply.started":"2024-04-15T14:03:47.419425Z","shell.execute_reply":"2024-04-15T14:03:47.875243Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"['__annotations__', '__builtins__', '__call__', '__class__', '__closure__', '__code__', '__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__get__', '__getattribute__', '__globals__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__kwdefaults__', '__le__', '__lt__', '__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **7.4 교차분석**\n","metadata":{}},{"cell_type":"code","source":"#교차분석 \n#적합도 거정 \n# 유의수준 0.05로 적합도 검정 수행\nfrom scipy.stats import chisquare\n# chi = chisquare(table, f_exp=[171,171])\n# print('<적합도 검정>\\n',chi)\n\n# 카이제곱 검정을 통한 독립성 검정 수행\nfrom scipy.stats import chi2_contingency\nimport pandas as pd\n# 데이터 불러오기\ndf = pd.read_csv(\"../data/titanic.csv\")\ntable = pd.crosstab(df['class'], df['survived'])\nchi, p, df, expect = chi2_contingency(table) \nprint('Statistic:', chi)\nprint('p-value:', p)\nprint('df:', df)\nprint('expect: \\n', expect)","metadata":{"execution":{"iopub.status.busy":"2024-04-15T14:03:15.287494Z","iopub.execute_input":"2024-04-15T14:03:15.289007Z","iopub.status.idle":"2024-04-15T14:03:16.497704Z","shell.execute_reply.started":"2024-04-15T14:03:15.288954Z","shell.execute_reply":"2024-04-15T14:03:16.495460Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터 불러오기\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../data/titanic.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m table \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mcrosstab(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m], df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m chi, p, df, expect \u001b[38;5;241m=\u001b[39m chi2_contingency(table) \n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1024\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1011\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1012\u001b[0m     dialect,\n\u001b[1;32m   1013\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[1;32m   1022\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1024\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:618\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    615\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    617\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 618\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1618\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1617\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1618\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1878\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1877\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1878\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1889\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/titanic.csv'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '../data/titanic.csv'","output_type":"error"}]},{"cell_type":"markdown","source":"## **7.5 회귀분석**","metadata":{}},{"cell_type":"code","source":"from statsmodels.formula.api import ols\n# y = house['price']\n# X = house[['sqft_living']]\n# # 단순선형회귀모형 적합\n# lr = ols('price ~ sqft_living',data=house).fit()\n# y_pred = lr.predict(X)\n# lr.summary()\n\nfrom patsy import dmatrices\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n# 독립변수와 종속변수를 데이터프레임으로 나누어 저장하는 함수 \n# y,X = dmatrices(\"Price ~ EngineSize + RPM + Weight+ Length + MPGcity + MPGhighway\",\n#                 data = Cars,return_type =\"dataframe\")\n# # 독립변수끼리의 VIF값을 계산하여 데이터프레임으로 만드는 과정 \n# vif_list = []\n# for i in range(1,len(X.columns)): \n#     vif_list.append([variance_inflation_factor(X.values,i), X.columns[i]])\n# pd.DataFrame(vif_list,columns=['vif','variable'])\n# model = smf.ols(formula =\"Price ~ EngineSize + RPM + Weight  + MPGhighway\", data = Cars)\n# result = model.fit()\n# result.summary()\n\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 전진 선택법 \nimport time\nimport itertools\ndef processSubset(X,y, feature_set):\n            model = sm.OLS(y,X[list(feature_set)]) # Modeling\n            regr = model.fit() # 모델 학습\n            AIC = regr.aic # 모델의 AIC\n            return {\"model\":regr, \"AIC\":AIC}\n# 전진선택법\ndef forward(X, y, predictors):\n    # 데이터 변수들이 미리정의된 predictors에 있는지 없는지 확인 및 분류\n    remaining_predictors = [p for p in X.columns.difference(['Intercept']) if p not in predictors]\n    results = []\n    for p in remaining_predictors:\n        results.append(processSubset(X=X, y= y,            feature_set=predictors+[p]+['Intercept']))\n        \n    # 데이터프레임으로 변환\n    models = pd.DataFrame(results)\n    # AIC가 가장 낮은 것을 선택\n    best_model = models.loc[models['AIC'].argmin()] # index\n    print(\"Processed \", models.shape[0], \"models on\", len(predictors)+1, \"predictors in\")\n    print('Selected predictors:',best_model['model'].model.exog_names,' AIC:',best_model[0] )\n    return best_model\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 후진소거법\ndef backward(X,y,predictors):\n    tic = time.time()\n    results = []\n    \n    # 데이터 변수들이 미리정의된 predictors 조합 확인\n    for combo in itertools.combinations(predictors, len(predictors) -1):\n        results.append(processSubset(X=X, y= y,        feature_set = list(combo)+['Intercept']))\n    models = pd.DataFrame(results)\n    \n    # 가장 낮은 AIC를 가진 모델을 선택\n    best_model = models.loc[models['AIC'].argmin()]\n    toc = time.time()\n    print(\"Processed \", models.shape[0], \"models on\",          len(predictors) -1, \"predictors in\", (toc - tic))\n    print('Selected predictors:',best_model['model'].model.exog_names,         'AIC:',best_model[0] )\n\n    return best_model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 단계적 선택법\ndef Stepwise_model(X,y):\n    Stepmodels = pd.DataFrame(columns=[\"AIC\", \"model\"])\n    tic = time.time()\n    predictors = []\n    Smodel_before = processSubset(X,y,predictors+['Intercept'])['AIC']\n\n    for i in range(1, len(X.columns.difference(['Intercept'])) +1):\n        Forward_result = forward(X=X, y=y, predictors=predictors) \n        print('forward')\n        Stepmodels.loc[i] = Forward_result\n        predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n        predictors = [ k for k in predictors if k !='Intercept']\n        Backward_result = backward(X=X, y=y, predictors=predictors)\n\n        if Backward_result['AIC']< Forward_result['AIC']:\n            Stepmodels.loc[i] = Backward_result\n            predictors = Stepmodels.loc[i][\"model\"].model.exog_names\n            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n            predictors = [ k for k in predictors if k !='Intercept']\n            print('backward')\n\n        if Stepmodels.loc[i]['AIC']> Smodel_before:\n            break\n        else:\n            Smodel_before = Stepmodels.loc[i][\"AIC\"]\n    toc = time.time()\n    print(\"Total elapsed time:\", (toc - tic), \"seconds.\")\n\n    return (Stepmodels['model'][len(Stepmodels['model'])])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Stepwise_best_model = Stepwise_model(X=X, y=y)\nStepwise_best_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **7.6 군집분석**","metadata":{}},{"cell_type":"code","source":"## 군집분석 \nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import calinski_harabasz_score\n# calinsk_harabasz_score\n# for k in range(2, 10):\n#     kmeans_model = KMeans(n_clusters=k, random_state=1).fit(X)\n#     labels = kmeans_model.labels_\n#     print(calinski_harabasz_score(X, labels))\n# elbow\n# for i in range(1, 11):\n#     km=KMeans(n_clusters=i, random_state=1)\n#     km.fit(X)\n#     sse.append(km.inertia_) # km.inertia_\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture \n# iris = pd.read_csv(\"../data/iris.csv\")\n# iris.info()\n# df = iris.drop(\"target\", axis=1)\n# scaler = StandardScaler()\n# df_scaled = scaler.fit_transform(df)\n\n# gmm = GaussianMixture(n_components=3)\n# gmm.fit(df_scaled)\n# gmm_labels = gmm.predict(df_scaled)\n# gmm_labels","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# statsmodels.formula.api vs statsmodels.api 의 차이는 \n 전자는 자동으로 상수항을 추가해 준다면, 후자는 x_new = sm.add_constant(x) 과 같이 처리해주고 SM_model_2 = sm.OLS(y, x_new).fit() 이렇게 해주어야 한다. \n https://michael-fuchs-python.netlify.app/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/","metadata":{}},{"cell_type":"markdown","source":"연관 분석(association analysis)에서 \"lift(향상도)\"는 상품 간의 연관성을 측정하는 데 사용되는 지표 중 하나입니다. \n<span style=\"color:red\"> 향상도는 두 개의 상품 간의 관계가 랜덤하게 발생한 경우에 비해 그 빈도가 얼마나 높은지를 측정합니다.</span>\n일반적으로 향상도는 다음과 같은 공식으로 계산됩니다:\n향상도의 해석은 다음과 같습니다:\n향상도가 1보다 크면, 두 상품 간의 연관 관계가 랜덤한 경우보다 더 자주 함께 발생한다는 것을 의미합니다. 이는 양의 상관 관계를 나타냅니다.\n향상도가 1에 가까우면, 두 상품 간의 연관성이 랜덤한 경우와 비슷하다는 것을 의미합니다.\n향상도가 1보다 작으면, 두 상품 간의 연관 관계가 랜덤한 경우보다 덜 자주 함께 발생한다는 것을 의미합니다. 이는 음의 상관 관계를 나타냅니다.\n따라서 향상도는 연관 규칙의 유용성을 평가하는 데 사용되며, 특정 상품이 다른 상품과 어떻게 관련되어 있는지를 파악하는 데 도움이 됩니다.","metadata":{}},{"cell_type":"markdown","source":"## **7.7 연관분석**","metadata":{}},{"cell_type":"code","source":"## 연관분석 \n#runstest_1samp, apriori, assocaiton_rules  \n# 연관성에 대한 검정 \nfrom statsmodels.sandbox.stats.runs import runstest_1samp\n# 연관규칙분석 \n#  Lift 해석은 1보다 크며, 연관이 \nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules","metadata":{"execution":{"iopub.status.busy":"2024-03-02T06:49:23.109501Z","iopub.execute_input":"2024-03-02T06:49:23.109848Z","iopub.status.idle":"2024-03-02T06:49:23.114605Z","shell.execute_reply.started":"2024-03-02T06:49:23.109821Z","shell.execute_reply":"2024-03-02T06:49:23.113290Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndf =pd.DataFrame(np.random.randint(100,1000,100))\nss = [[\"a\",\"b\",\"c\"],[\"d\",\"e\"],[\"a\",\"b\",\"c\",\"d\",\"e\"]]\n\nfrom  mlxtend.preprocessing import TransactionEncoder\nte = TransactionEncoder()\nte_ary = te.fit_transform(ss)\n# print(help(te))\nprint(type(te), te.columns_, te_ary)\nprint(te.inverse_transform(te_ary))\n\ndf = pd.DataFrame(te_ary, columns = te.columns_)\nprint(df)\n\n# df.plot()\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-02T07:30:48.754291Z","iopub.execute_input":"2024-03-02T07:30:48.754866Z","iopub.status.idle":"2024-03-02T07:30:48.763905Z","shell.execute_reply.started":"2024-03-02T07:30:48.754843Z","shell.execute_reply":"2024-03-02T07:30:48.762949Z"},"trusted":true},"execution_count":71,"outputs":[{"name":"stdout","text":"<class 'mlxtend.preprocessing.transactionencoder.TransactionEncoder'> ['a', 'b', 'c', 'd', 'e'] [[ True  True  True False False]\n [False False False  True  True]\n [ True  True  True  True  True]]\n[['a', 'b', 'c'], ['d', 'e'], ['a', 'b', 'c', 'd', 'e']]\n       a      b      c      d      e\n0   True   True   True  False  False\n1  False  False  False   True   True\n2   True   True   True   True   True\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **7.8 시계열 분석**","metadata":{}},{"cell_type":"code","source":"# 7-8.시계열, 이원분석 anova, lm_anova, tsa,\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n# ts = data\n# result = seasonal_decompose(ts, model='multiplicative')\n# plt.rcParams['figure.figsize'] = [12, 8]\n# result.plot()\n# plt.show()\n\n## 정상성 \nfrom statsmodels.tsa.stattools import adfuller\n# training = data[:\"2016-12-01\"]\n# test = data.drop(training.index)\n# print(help(adfuller))\n\n# 정상성 검사 \nadf = adfuller(training, regression='ct')\nprint(adf)\nfrom statsmodels.api import PCA, OLS, MANOVA\nfrom statsmodels.formula.api import ols \nimport statsmodels.tsa \nfrom statsmodels.tsa import arima,api \n\n# PACF, ACF\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\n\nfrom statsmodels.tsa.arima.model import ARIMA\n# model = ARIMA(training, order=(2,1,2))\n# res = model.fit()\n# res.summary()\n# plt.plot(res.predict())\n# plt.plot(training)\n# plt.legend([\"P\",\"R\"])\n# plt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 7-8.\nfrom pmdarima import auto_arima\nauto_model = auto_arima(training, start_p=0, d=1, start_q=0,\n                        max_p=3, max_q=3, \n                        start_P=0, start_Q=0,\n                        max_P=3, max_Q=3, m=12,\n                        seasonal=True, information_criterion='aic',\n                        trace=True)\n# m =12면 달 기준, m  =1 이면 년 단위 \nauto_model.summary()\nauto_pred_y= pd.DataFrame(auto_model.predict(n_periods=len(test)), \n                          index=test.index) \nauto_pred_y.columns = ['predicted_price']\nauto_pred_y\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint(\"r2_score : \", r2_score(test_y, auto_pred_y))\nRMSE = mean_squared_error(test_y, auto_pred_y)**0.5\nprint(\"RMSE : \" , RMSE)","metadata":{},"execution_count":null,"outputs":[]}]}