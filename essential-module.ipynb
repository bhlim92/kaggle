{
 "cells": [
  {
   "cell_type": "raw",
   "id": "40b6e925",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.003418,
     "end_time": "2024-03-02T07:18:40.283672",
     "exception": false,
     "start_time": "2024-03-02T07:18:40.280254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cef782",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T07:18:40.294799Z",
     "iopub.status.busy": "2024-03-02T07:18:40.293792Z",
     "iopub.status.idle": "2024-03-02T07:18:45.956134Z",
     "shell.execute_reply": "2024-03-02T07:18:45.954837Z"
    },
    "papermill": {
     "duration": 5.671863,
     "end_time": "2024-03-02T07:18:45.959227",
     "exception": false,
     "start_time": "2024-03-02T07:18:40.287364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 기본적인 모듈, EDA시 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn \n",
    "\n",
    "# 전처리 \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 통계 처리 \n",
    "import scipy.stats as st \n",
    "from statsmodels.tsa import arima,api \n",
    "\n",
    "\n",
    "# 머신러닝 - 회귀분석 \n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# 머시너링 - 앙상블 \n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "# 머신러닝 - SVM \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "\n",
    "# 머신러닝 - 평가 지표 \n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score\n",
    "\n",
    "# 시계열, 이원분석 anova, lm_anova, tsa, \n",
    "from statsmodels.api import PCA, OLS, MANOVA\n",
    "from statsmodels.formula.api import ols \n",
    "import statsmodels.tsa \n",
    "from statsmodels.tsa import arima,api \n",
    "\n",
    "ll = [i for i in dir(sklearn.metrics) if \"error\" in i ]\n",
    "stats = [i for i in dir(statsmodels.tsa)]\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4e0b8",
   "metadata": {},
   "source": [
    "# New beginning ###\n",
    "새로운 시작에는 항상 가속도를 발생시키는 힘이 필요하다 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb0702",
   "metadata": {
    "papermill": {
     "duration": 0.002388,
     "end_time": "2024-03-02T07:18:45.964511",
     "exception": false,
     "start_time": "2024-03-02T07:18:45.962123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# statsmodels.formula.api vs statsmodels.api 의 차이는 \n",
    " 전자는 자동으로 상수항을 추가해 준다면, 후자는 x_new = sm.add_constant(x) 과 같이 처리해주고 SM_model_2 = sm.OLS(y, x_new).fit() 이렇게 해주어야 한다. \n",
    " https://michael-fuchs-python.netlify.app/2019/07/02/statsmodel-formula-api-vs-statsmodel-api/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880dfa79",
   "metadata": {
    "papermill": {
     "duration": 0.00229,
     "end_time": "2024-03-02T07:18:45.969432",
     "exception": false,
     "start_time": "2024-03-02T07:18:45.967142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "연관 분석(association analysis)에서 \"lift(향상도)\"는 상품 간의 연관성을 측정하는 데 사용되는 지표 중 하나입니다. \n",
    "<span style=\"color:red\"> 향상도는 두 개의 상품 간의 관계가 랜덤하게 발생한 경우에 비해 그 빈도가 얼마나 높은지를 측정합니다.</span>\n",
    "일반적으로 향상도는 다음과 같은 공식으로 계산됩니다:\n",
    "향상도의 해석은 다음과 같습니다:\n",
    "향상도가 1보다 크면, 두 상품 간의 연관 관계가 랜덤한 경우보다 더 자주 함께 발생한다는 것을 의미합니다. 이는 양의 상관 관계를 나타냅니다.\n",
    "향상도가 1에 가까우면, 두 상품 간의 연관성이 랜덤한 경우와 비슷하다는 것을 의미합니다.\n",
    "향상도가 1보다 작으면, 두 상품 간의 연관 관계가 랜덤한 경우보다 덜 자주 함께 발생한다는 것을 의미합니다. 이는 음의 상관 관계를 나타냅니다.\n",
    "따라서 향상도는 연관 규칙의 유용성을 평가하는 데 사용되며, 특정 상품이 다른 상품과 어떻게 관련되어 있는지를 파악하는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6492c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-02T07:18:45.977629Z",
     "iopub.status.busy": "2024-03-02T07:18:45.976315Z",
     "iopub.status.idle": "2024-03-02T07:18:45.995507Z",
     "shell.execute_reply": "2024-03-02T07:18:45.994105Z"
    },
    "papermill": {
     "duration": 0.026808,
     "end_time": "2024-03-02T07:18:45.998855",
     "exception": false,
     "start_time": "2024-03-02T07:18:45.972047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## runstest_1samp, apriori, assocaiton_rules  \n",
    "## 연관성에 대한 검정 \n",
    "from statsmodels.sandbox.stats.runs import runstest_1samp\n",
    "# 연관규칙분석 \n",
    "#  Lift 해석은 1보다 크며, 연관이 \n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "print(\"hi it's association rules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de3aba",
   "metadata": {},
   "source": [
    "# 빅데이터 기사에 사용되는 모듈로 \n",
    " - sklearn.model_selection \n",
    " - sklearn.linear_model \n",
    " - skelarn.ensemble\n",
    " - statsmodels.api as sm\n",
    " - statsmodels.formula.api as ols \n",
    " - scipy.stats \n",
    " - sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b830be97",
   "metadata": {
    "papermill": {
     "duration": 0.002759,
     "end_time": "2024-03-02T07:18:46.004687",
     "exception": false,
     "start_time": "2024-03-02T07:18:46.001928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_wine in module sklearn.datasets._base:\n",
      "\n",
      "load_wine(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the wine dataset (classification).\n",
      "    \n",
      "    .. versionadded:: 0.18\n",
      "    \n",
      "    The wine dataset is a classic and very easy multi-class classification\n",
      "    dataset.\n",
      "    \n",
      "    =================   ==============\n",
      "    Classes                          3\n",
      "    Samples per class        [59,71,48]\n",
      "    Samples total                  178\n",
      "    Dimensionality                  13\n",
      "    Features            real, positive\n",
      "    =================   ==============\n",
      "    \n",
      "    The copy of UCI ML Wine Data Set dataset is downloaded and modified to fit\n",
      "    standard format from:\n",
      "    https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "    \n",
      "    Read more in the :ref:`User Guide <wine_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (178, 13)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (178,)\n",
      "            The classification target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        target_names: list\n",
      "            The names of target classes.\n",
      "        frame: DataFrame of shape (178, 14)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "        A tuple of two ndarrays by default. The first contains a 2D array of shape\n",
      "        (178, 13) with each row representing one sample and each column representing\n",
      "        the features. The second array of shape (178,) contains the target samples.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Let's say you are interested in the samples 10, 80, and 140, and want to\n",
      "    know their class name.\n",
      "    \n",
      "    >>> from sklearn.datasets import load_wine\n",
      "    >>> data = load_wine()\n",
      "    >>> data.target[[10, 80, 140]]\n",
      "    array([0, 1, 2])\n",
      "    >>> list(data.target_names)\n",
      "    ['class_0', 'class_1', 'class_2']\n",
      "\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'> <class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Data columns (total 13 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   alcohol                       178 non-null    float64\n",
      " 1   malic_acid                    178 non-null    float64\n",
      " 2   ash                           178 non-null    float64\n",
      " 3   alcalinity_of_ash             178 non-null    float64\n",
      " 4   magnesium                     178 non-null    float64\n",
      " 5   total_phenols                 178 non-null    float64\n",
      " 6   flavanoids                    178 non-null    float64\n",
      " 7   nonflavanoid_phenols          178 non-null    float64\n",
      " 8   proanthocyanins               178 non-null    float64\n",
      " 9   color_intensity               178 non-null    float64\n",
      " 10  hue                           178 non-null    float64\n",
      " 11  od280/od315_of_diluted_wines  178 non-null    float64\n",
      " 12  proline                       178 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 18.2 KB\n",
      "<class 'pandas.core.series.Series'>\n",
      "RangeIndex: 178 entries, 0 to 177\n",
      "Series name: target\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "178 non-null    int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 840.0 bytes\n",
      "<class 'pandas.core.series.Series'>\n",
      "Index: 124 entries, 163 to 135\n",
      "Series name: target\n",
      "Non-Null Count  Dtype\n",
      "--------------  -----\n",
      "124 non-null    int32\n",
      "dtypes: int32(1)\n",
      "memory usage: 1.5 KB\n",
      "None count    124.000000\n",
      "mean       0.935484\n",
      "std        0.772936\n",
      "min        0.000000\n",
      "25%        0.000000\n",
      "50%        1.000000\n",
      "75%        2.000000\n",
      "max        2.000000\n",
      "Name: target, dtype: float64 target\n",
      "1    50\n",
      "0    41\n",
      "2    33\n",
      "Name: count, dtype: int64 target\n",
      "1    21\n",
      "0    18\n",
      "2    15\n",
      "Name: count, dtype: int64 target\n",
      "1    71\n",
      "0    59\n",
      "2    48\n",
      "Name: count, dtype: int64\n",
      "Help on function concat in module pandas.core.reshape.concat:\n",
      "\n",
      "concat(objs: 'Iterable[NDFrame] | Mapping[HashableT, NDFrame]', *, axis: 'Axis' = 0, join: 'str' = 'outer', ignore_index: 'bool' = False, keys=None, levels=None, names=None, verify_integrity: 'bool' = False, sort: 'bool' = False, copy: 'bool | None' = None) -> 'DataFrame | Series'\n",
      "    Concatenate pandas objects along a particular axis.\n",
      "    \n",
      "    Allows optional set logic along the other axes.\n",
      "    \n",
      "    Can also add a layer of hierarchical indexing on the concatenation axis,\n",
      "    which may be useful if the labels are the same (or overlapping) on\n",
      "    the passed axis number.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    objs : a sequence or mapping of Series or DataFrame objects\n",
      "        If a mapping is passed, the sorted keys will be used as the `keys`\n",
      "        argument, unless it is passed, in which case the values will be\n",
      "        selected (see below). Any None objects will be dropped silently unless\n",
      "        they are all None in which case a ValueError will be raised.\n",
      "    axis : {0/'index', 1/'columns'}, default 0\n",
      "        The axis to concatenate along.\n",
      "    join : {'inner', 'outer'}, default 'outer'\n",
      "        How to handle indexes on other axis (or axes).\n",
      "    ignore_index : bool, default False\n",
      "        If True, do not use the index values along the concatenation axis. The\n",
      "        resulting axis will be labeled 0, ..., n - 1. This is useful if you are\n",
      "        concatenating objects where the concatenation axis does not have\n",
      "        meaningful indexing information. Note the index values on the other\n",
      "        axes are still respected in the join.\n",
      "    keys : sequence, default None\n",
      "        If multiple levels passed, should contain tuples. Construct\n",
      "        hierarchical index using the passed keys as the outermost level.\n",
      "    levels : list of sequences, default None\n",
      "        Specific levels (unique values) to use for constructing a\n",
      "        MultiIndex. Otherwise they will be inferred from the keys.\n",
      "    names : list, default None\n",
      "        Names for the levels in the resulting hierarchical index.\n",
      "    verify_integrity : bool, default False\n",
      "        Check whether the new concatenated axis contains duplicates. This can\n",
      "        be very expensive relative to the actual data concatenation.\n",
      "    sort : bool, default False\n",
      "        Sort non-concatenation axis if it is not already aligned.\n",
      "    \n",
      "    copy : bool, default True\n",
      "        If False, do not copy data unnecessarily.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    object, type of objs\n",
      "        When concatenating all ``Series`` along the index (axis=0), a\n",
      "        ``Series`` is returned. When ``objs`` contains at least one\n",
      "        ``DataFrame``, a ``DataFrame`` is returned. When concatenating along\n",
      "        the columns (axis=1), a ``DataFrame`` is returned.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.join : Join DataFrames using indexes.\n",
      "    DataFrame.merge : Merge DataFrames by indexes or columns.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The keys, levels, and names arguments are all optional.\n",
      "    \n",
      "    A walkthrough of how this method fits in with other tools for combining\n",
      "    pandas objects can be found `here\n",
      "    <https://pandas.pydata.org/pandas-docs/stable/user_guide/merging.html>`__.\n",
      "    \n",
      "    It is not recommended to build DataFrames by adding single rows in a\n",
      "    for loop. Build a list of rows and make a DataFrame in a single concat.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Combine two ``Series``.\n",
      "    \n",
      "    >>> s1 = pd.Series(['a', 'b'])\n",
      "    >>> s2 = pd.Series(['c', 'd'])\n",
      "    >>> pd.concat([s1, s2])\n",
      "    0    a\n",
      "    1    b\n",
      "    0    c\n",
      "    1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Clear the existing index and reset it in the result\n",
      "    by setting the ``ignore_index`` option to ``True``.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], ignore_index=True)\n",
      "    0    a\n",
      "    1    b\n",
      "    2    c\n",
      "    3    d\n",
      "    dtype: object\n",
      "    \n",
      "    Add a hierarchical index at the outermost level of\n",
      "    the data with the ``keys`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2'])\n",
      "    s1  0    a\n",
      "        1    b\n",
      "    s2  0    c\n",
      "        1    d\n",
      "    dtype: object\n",
      "    \n",
      "    Label the index keys you create with the ``names`` option.\n",
      "    \n",
      "    >>> pd.concat([s1, s2], keys=['s1', 's2'],\n",
      "    ...           names=['Series name', 'Row ID'])\n",
      "    Series name  Row ID\n",
      "    s1           0         a\n",
      "                 1         b\n",
      "    s2           0         c\n",
      "                 1         d\n",
      "    dtype: object\n",
      "    \n",
      "    Combine two ``DataFrame`` objects with identical columns.\n",
      "    \n",
      "    >>> df1 = pd.DataFrame([['a', 1], ['b', 2]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df1\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    >>> df2 = pd.DataFrame([['c', 3], ['d', 4]],\n",
      "    ...                    columns=['letter', 'number'])\n",
      "    >>> df2\n",
      "      letter  number\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    >>> pd.concat([df1, df2])\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return everything. Columns outside the intersection will\n",
      "    be filled with ``NaN`` values.\n",
      "    \n",
      "    >>> df3 = pd.DataFrame([['c', 3, 'cat'], ['d', 4, 'dog']],\n",
      "    ...                    columns=['letter', 'number', 'animal'])\n",
      "    >>> df3\n",
      "      letter  number animal\n",
      "    0      c       3    cat\n",
      "    1      d       4    dog\n",
      "    >>> pd.concat([df1, df3], sort=False)\n",
      "      letter  number animal\n",
      "    0      a       1    NaN\n",
      "    1      b       2    NaN\n",
      "    0      c       3    cat\n",
      "    1      d       4    dog\n",
      "    \n",
      "    Combine ``DataFrame`` objects with overlapping columns\n",
      "    and return only those that are shared by passing ``inner`` to\n",
      "    the ``join`` keyword argument.\n",
      "    \n",
      "    >>> pd.concat([df1, df3], join=\"inner\")\n",
      "      letter  number\n",
      "    0      a       1\n",
      "    1      b       2\n",
      "    0      c       3\n",
      "    1      d       4\n",
      "    \n",
      "    Combine ``DataFrame`` objects horizontally along the x axis by\n",
      "    passing in ``axis=1``.\n",
      "    \n",
      "    >>> df4 = pd.DataFrame([['bird', 'polly'], ['monkey', 'george']],\n",
      "    ...                    columns=['animal', 'name'])\n",
      "    >>> pd.concat([df1, df4], axis=1)\n",
      "      letter  number  animal    name\n",
      "    0      a       1    bird   polly\n",
      "    1      b       2  monkey  george\n",
      "    \n",
      "    Prevent the result from including duplicate index values with the\n",
      "    ``verify_integrity`` option.\n",
      "    \n",
      "    >>> df5 = pd.DataFrame([1], index=['a'])\n",
      "    >>> df5\n",
      "       0\n",
      "    a  1\n",
      "    >>> df6 = pd.DataFrame([2], index=['a'])\n",
      "    >>> df6\n",
      "       0\n",
      "    a  2\n",
      "    >>> pd.concat([df5, df6], verify_integrity=True)\n",
      "    Traceback (most recent call last):\n",
      "        ...\n",
      "    ValueError: Indexes have overlapping values: ['a']\n",
      "    \n",
      "    Append a single row to the end of a ``DataFrame`` object.\n",
      "    \n",
      "    >>> df7 = pd.DataFrame({'a': 1, 'b': 2}, index=[0])\n",
      "    >>> df7\n",
      "        a   b\n",
      "    0   1   2\n",
      "    >>> new_row = pd.Series({'a': 3, 'b': 4})\n",
      "    >>> new_row\n",
      "    a    3\n",
      "    b    4\n",
      "    dtype: int64\n",
      "    >>> pd.concat([df7, new_row.to_frame().T], ignore_index=True)\n",
      "        a   b\n",
      "    0   1   2\n",
      "    1   3   4\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_diabetes, load_wine, load_breast_cancer\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(help(load_wine))\n",
    "x,y  = load_wine(return_X_y=True, as_frame=True )\n",
    "\n",
    "print(type(x), type(y))\n",
    "\n",
    "x.info()\n",
    "y.info()\n",
    "# return\n",
    "# print(t.shape, t.columns)\n",
    "\n",
    "# print(t.isnull().sum())\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# x = t.drop([\"target\"], axis = 1)\n",
    "# y = pd.DataFrame(t[\"target\"],columns=[\"target\"])\n",
    "# print(x.info())\n",
    "# print(help(train_test_split))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y, test_size=0.3,\n",
    "                                                  random_state=2023, stratify=y)\n",
    "# print(x_train.info())\n",
    "print(y_train.info(), y_train.describe(), y_train.value_counts(), y_val.value_counts(), y.value_counts())\n",
    "\n",
    "## 유형1\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "print(help(pd.concat))\n",
    "\n",
    "# 유형2\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import *\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, precision_score,f1_score\n",
    "\n",
    "# 유형3\n",
    "from scipy.stats import chisquare, chi2_contingency, ttest_1samp, ttest_rel, ttest_ind, f_oneway\n",
    "# from scipy.stats import wilcoxn, ranksums\n",
    "from scipy.stats import ranksums\n",
    "\n",
    "from scipy.stats import shapiro, bartlett\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as ols\n",
    "from statsmodels.stats.anova import anova_lm\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.117668,
   "end_time": "2024-03-02T07:18:47.032952",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-02T07:18:36.915284",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
